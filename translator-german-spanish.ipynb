{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# german_file_path=\"Data_Sets/deu.txt\"\n",
    "# spanish_file_path=\"Data_Sets/data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store German and English data\n",
    "# german_data = []\n",
    "# english_data = []\n",
    "\n",
    "# with open(german_file_path,'r',encoding='utf-8') as file:\n",
    "#     for line in file:\n",
    "#         english,german,_=line.strip().split('\\t')\n",
    "#         english_data.append(english)\n",
    "#         german_data.append(german)\n",
    "        \n",
    "\n",
    "# # Create a DataFrame using Pandas\n",
    "# df = pd.DataFrame({ 'English': english_data , 'German': german_data})\n",
    "\n",
    "# # Display the first few rows of the DataFrame\n",
    "# print(df[:20])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spanish=pd.read_csv(spanish_file_path)\n",
    "\n",
    "# # renaming the names\n",
    "# new_column_names={'english':'English','spanish':'Spanish'}\n",
    "# df_spanish=df_spanish.rename(columns=new_column_names)\n",
    "\n",
    "# print(df_spanish[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name=f\"data_german.csv\"\n",
    "\n",
    "# df.to_csv(f\"Data_Sets/{file_name}\",index=False)\n",
    "# print(\"DataFrame saved successfully as\", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# current_file_path=\"Data_Sets/data.csv\"\n",
    "# new_file_path=\"Data_Sets/data_spanish.csv\"\n",
    "\n",
    "# os.rename(current_file_path,new_file_path)\n",
    "\n",
    "# print(\"File renamed successfully to\", new_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "german_file_path=\"Data_Sets/data_german.csv\"\n",
    "spanish_file_path=\"Data_Sets/data_spanish.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(german_file_path)\n",
    "df2=pd.read_csv(spanish_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "import csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>German</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Potzdonner!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Donnerwetter!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fire!</td>\n",
       "      <td>Feuer!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Help!</td>\n",
       "      <td>Hilfe!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Help!</td>\n",
       "      <td>Zu Hülf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Stop!</td>\n",
       "      <td>Stopp!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Wait!</td>\n",
       "      <td>Warte!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Wait.</td>\n",
       "      <td>Warte.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Begin.</td>\n",
       "      <td>Fang an.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Go on.</td>\n",
       "      <td>Mach weiter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hello!</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hurry!</td>\n",
       "      <td>Beeil dich!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Hurry!</td>\n",
       "      <td>Schnell!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I hid.</td>\n",
       "      <td>Ich versteckte mich.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I hid.</td>\n",
       "      <td>Ich habe mich versteckt.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   English                    German\n",
       "0      Go.                      Geh.\n",
       "1      Hi.                    Hallo!\n",
       "2      Hi.                Grüß Gott!\n",
       "3     Run!                     Lauf!\n",
       "4     Run.                     Lauf!\n",
       "5     Wow!               Potzdonner!\n",
       "6     Wow!             Donnerwetter!\n",
       "7    Fire!                    Feuer!\n",
       "8    Help!                    Hilfe!\n",
       "9    Help!                  Zu Hülf!\n",
       "10   Stop!                    Stopp!\n",
       "11   Wait!                    Warte!\n",
       "12   Wait.                    Warte.\n",
       "13  Begin.                  Fang an.\n",
       "14  Go on.              Mach weiter.\n",
       "15  Hello!                    Hallo!\n",
       "16  Hurry!               Beeil dich!\n",
       "17  Hurry!                  Schnell!\n",
       "18  I hid.      Ich versteckte mich.\n",
       "19  I hid.  Ich habe mich versteckt."
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a prank, some students let three goats loose inside their school after painting the numbers 1, 2 and 4 on the sides of the goats. The teachers spent most of the day looking for goat number 3.\n",
      "The small crowd at Hiroshima Peace Memorial Park stood for a moment of silence at 8:15 a.m., the exact moment an atomic bomb nicknamed “Little Boy” was dropped from the U.S. warplane Enola Gay.\n",
      "In today's world, we have to equip all our kids with an education that prepares them for success, regardless of what they look like, or how much their parents make, or the zip code that they live in.\n",
      "Death is something that we're often discouraged to talk about or even think about, but I've realized that preparing for death is one of the most empowering things you can do. Thinking about death clarifies your life.\n",
      "At a moment when our economy is growing, our businesses are creating jobs at the fastest pace since the 1990s, and wages are starting to rise again, we have to make some choices about the kind of country we want to be.\n",
      "Even if some sentences by non-native speakers are good, it's really hard to trust that they are good, so members would be helping us much more by limiting their contributions to sentences in their own native languages.\n",
      "If someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.\n",
      "If someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.\n",
      "If someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.\n",
      "Doubtless there exists in this world precisely the right woman for any given man to marry and vice versa; but when you consider that a human being has the opportunity of being acquainted with only a few hundred people, and out of the few hundred that there are but a dozen or less whom he knows intimately, and out of the dozen, one or two friends at most, it will easily be seen, when we remember the number of millions who inhabit this world, that probably, since the earth was created, the right man has never yet met the right woman.\n"
     ]
    }
   ],
   "source": [
    "first_column_values_last_10_rows=df1['English'].tail(10)\n",
    "\n",
    "\n",
    "paragraph = '\\n'.join(first_column_values_last_10_rows)\n",
    "# Print the paragraph\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221533"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the English and german translation pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common function for both spanish and german\n",
    "\n",
    "class split_pairs:\n",
    " def split_pairs_method(self,df1): \n",
    "  text_pairs=[]\n",
    "  i=0\n",
    "  for i in range(len(df1)):\n",
    "    english,language=df1[\"English\"][i],df1[\"German\"][i]\n",
    "    german=\"[start] \"+language+\" [end]\"\n",
    "    text_pairs.append((english,language))\n",
    "\n",
    "  return text_pairs    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly selecting that if the above function work\n",
    "class random_pair_test:\n",
    "  def random_test_method(self,text_pairs): \n",
    "   for i in range(3):\n",
    "    print(random.choice(text_pairs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('My father will kill me.', 'Mein Vater wird mich umbringen.')\n",
      "(\"I have a doctor's appointment.\", 'Ich habe einen Termin beim Arzt.')\n",
      "('A time bomb went off in the airport killing thirteen people.', 'Eine Zeitbombe explodierte im Flughafen und tötete dreizehn Menschen.')\n"
     ]
    }
   ],
   "source": [
    "#pairing for german text\n",
    "\n",
    "german_text_pairs=split_pairs().split_pairs_method(df1)\n",
    "\n",
    "random_pair_test().random_test_method(german_text_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(german_text_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into training, validation,testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for splitting text pairs in to train,test,validation\n",
    "class splitting:\n",
    "    def splitting_method(self,text_pairs):\n",
    "        num_val_sample=int(0.15*len(text_pairs))\n",
    "        num_train_samples=len(text_pairs) - 2 * num_val_sample\n",
    "        train_pairs=text_pairs[:num_train_samples]\n",
    "        val_pairs=text_pairs[num_train_samples:num_train_samples+num_val_sample]\n",
    "        test_pairs=text_pairs[num_train_samples+num_val_sample:]\n",
    "        \n",
    "        print(\"Total Sentences: \",len(text_pairs))\n",
    "        print(\"Training set size: \",len(train_pairs))\n",
    "        print(\"Validation set size: \",len(val_pairs))\n",
    "        print(\"Testng set size: \",len(test_pairs))\n",
    "        return train_pairs,val_pairs,test_pairs    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences:  221533\n",
      "Training set size:  155075\n",
      "Validation set size:  33229\n",
      "Testng set size:  33229\n"
     ]
    }
   ],
   "source": [
    "german_train_pairs,german_val_pairs,german_test_pairs=splitting().splitting_method(german_text_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221533\n"
     ]
    }
   ],
   "source": [
    "print(len(german_test_pairs)+len(german_train_pairs)+len(german_val_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"I've got a bit of an ache in my back.\", 'Mein Rücken schmerzt etwas.')\n"
     ]
    }
   ],
   "source": [
    "print(german_val_pairs[200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Puncuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "\n",
    "f\"[{re.escape(strip_chars)}]\"\n",
    "\n",
    "f\"{5+3}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing the English and spanish text pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "Using a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m train_english_texts\u001b[38;5;241m=\u001b[39m[pair[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m german_train_pairs]\n\u001b[0;32m     22\u001b[0m train_german_texts\u001b[38;5;241m=\u001b[39m[pair[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m german_train_pairs]\n\u001b[1;32m---> 24\u001b[0m \u001b[43msource_vectorization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_english_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m target_vectorization\u001b[38;5;241m.\u001b[39madapt(train_german_texts)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\text_vectorization.py:431\u001b[0m, in \u001b[0;36mTextVectorization.adapt\u001b[1;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[0;32m    429\u001b[0m         data \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(data, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_state(data)\n\u001b[1;32m--> 431\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\text_vectorization.py:437\u001b[0m, in \u001b[0;36mTextVectorization.finalize_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfinalize_state\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 437\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lookup_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\index_lookup.py:625\u001b[0m, in \u001b[0;36mIndexLookup.finalize_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfinalize_state\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_input_vocabulary \u001b[38;5;129;01mor\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mequal(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_counts\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    626\u001b[0m         \u001b[38;5;66;03m# Finalize idf_weights to a const for call even if we don't need to\u001b[39;00m\n\u001b[0;32m    627\u001b[0m         \u001b[38;5;66;03m# compute a new vocabulary.\u001b[39;00m\n\u001b[0;32m    628\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_idf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    629\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midf_weights_const \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midf_weights\u001b[38;5;241m.\u001b[39mvalue()\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor.py:660\u001b[0m, in \u001b[0;36mTensor.__bool__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=invalid-bool-returned\u001b[39;00m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Dummy method to prevent a tensor from being used as a Python `bool`.\u001b[39;00m\n\u001b[0;32m    644\u001b[0m \n\u001b[0;32m    645\u001b[0m \u001b[38;5;124;03m  This overload raises a `TypeError` when the user inadvertently\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;124;03m    `TypeError`.\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 660\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_disallow_bool_casting\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor.py:316\u001b[0m, in \u001b[0;36mTensor._disallow_bool_casting\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_disallow_bool_casting\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 316\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_disallow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUsing a symbolic `tf.Tensor` as a Python `bool`\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor.py:303\u001b[0m, in \u001b[0;36mTensor._disallow\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_disallow\u001b[39m(\u001b[38;5;28mself\u001b[39m, task):\n\u001b[1;32m--> 303\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOperatorNotAllowedInGraphError(\n\u001b[0;32m    304\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not allowed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m You can attempt the following resolutions to the problem:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m If you are running in Graph mode, use Eager execution mode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or decorate this function with @tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m If you are using AutoGraph, you can try decorating this function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with @tf.function. If that does not work, then you may be using\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    310\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m an unsupported feature or your source code may not be visible\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    311\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to AutoGraph. See\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    312\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    313\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m: Using a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information."
     ]
    }
   ],
   "source": [
    "def custom_standardization(input_string):\n",
    "    lowercase=tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase,f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "vocab_size=15000\n",
    "sequence_length=20\n",
    "\n",
    "source_vectorization=layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "target_vectorization=layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length+1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "train_english_texts=[pair[0] for pair in german_train_pairs]\n",
    "train_german_texts=[pair[1] for pair in german_train_pairs]\n",
    "\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_german_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meeting will be held at 10:30 a.m. on Saturday.\n",
      "Die Versammlung wird am Samstag um zehn Uhr dreißig stattfinden.\n",
      "<TextVectorization name=text_vectorization_2, built=False>\n"
     ]
    }
   ],
   "source": [
    "print(train_english_texts[1])\n",
    "print(train_german_texts[1])\n",
    "\n",
    "print(source_vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The meeting will be held at 10:30 a.m. on Saturday.', 'Die Versammlung wird am Samstag um zehn Uhr dreißig stattfinden.'), (\"We're not doing anything.\", 'Wir machen nichts.'), (\"He's aware of his own faults.\", 'Er ist sich seiner eigenen Fehler bewusst.'), ('Have a nice day.', 'Einen schönen Tag.')]\n"
     ]
    }
   ],
   "source": [
    "print(german_train_pairs[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing datasets for the translation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['german'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "\n",
    "def format_dataset(eng,ger):\n",
    "    eng=source_vectorization(eng)\n",
    "    ger=target_vectorization(ger)\n",
    "    return ({\"english\":eng,\n",
    "             \"german\":ger[:,:-1],\n",
    "             },ger[:,1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts,ger_texts=zip(*pairs)\n",
    "    eng_texts=list(eng_texts)\n",
    "    ger_texts=list(ger_texts)\n",
    "    dataset=tf.data.Dataset.from_tensor_slices((eng_texts,ger_texts))\n",
    "    dataset=dataset.batch(batch_size)\n",
    "    dataset=dataset.map(format_dataset,num_parallel_calls=4)\n",
    "    \n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_data=make_dataset(german_train_pairs)\n",
    "val_data=make_dataset(german_val_pairs)\n",
    "\n",
    "# print(train_data)\n",
    "# print(val_data)\n",
    "\n",
    "for inputs, targets in train_data.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs['german'].shape: {inputs['german'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'english': array([[   6,   44,    3, ...,    0,    0,    0],\n",
      "       [  66, 1853,    2, ...,    0,    0,    0],\n",
      "       [   6,   80,  549, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [   5, 7854, 1688, ...,    0,    0,    0],\n",
      "       [   5,  425,    8, ...,    0,    0,    0],\n",
      "       [  31,    7,  121, ...,    0,    0,    0]], dtype=int64), 'german': array([[   2,  277,  141, ...,    0,    0,    0],\n",
      "       [   7,   34,    3, ...,    0,    0,    0],\n",
      "       [   2, 4346,   60, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [  11, 2431, 5594, ...,    0,    0,    0],\n",
      "       [  11,  417,    5, ...,    0,    0,    0],\n",
      "       [   2,   39,   75, ...,    0,    0,    0]], dtype=int64)}, array([[ 277,  141, 1541, ...,    0,    0,    0],\n",
      "       [  34,    3,   52, ...,    0,    0,    0],\n",
      "       [4346,   60, 4942, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [2431, 5594,   15, ...,    0,    0,    0],\n",
      "       [ 417,    5,   15, ...,    0,    0,    0],\n",
      "       [  39,   75, 2384, ...,    0,    0,    0]], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(list(train_data.as_numpy_iterator())[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers encoder implemented as a subclassed Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self,embed_dim,dense_dim,num_heads,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim=embed_dim\n",
    "        self.dense_dim=dense_dim\n",
    "        self.num_heads=num_heads\n",
    "        self.attention=layers.MultiHeadAttention(\n",
    "           num_heads=num_heads,key_dim=embed_dim)\n",
    "        self.dense_proj=keras.Sequential(\n",
    "            [layers.Dense(dense_dim,activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1=layers.LayerNormalization()\n",
    "        self.layernorm_2=layers.LayerNormalization()\n",
    "\n",
    "    def call(self,inputs,mask=None):\n",
    "        if mask is not None:\n",
    "            mask=mask[:,tf.newaxis,:]\n",
    "        attention_output=self.attention(\n",
    "            inputs,inputs,attention_mask=mask\n",
    "        )\n",
    "        project_input=self.layernorm_1(inputs+attention_output)\n",
    "        project_output=self.dense_proj(project_input)\n",
    "\n",
    "        return self.layernorm_2(project_input+project_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config=super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer decorder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self,embed_dim,dense_dim,num_heads,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim=embed_dim\n",
    "        self.dense_dim=dense_dim\n",
    "        self.num_heads=num_heads\n",
    "        self.attention_1=layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,key_dim=embed_dim)\n",
    "        self.attention_2=layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,key_dim=embed_dim)\n",
    "        self.dense_proj=keras.Sequential(\n",
    "            [layers.Dense(dense_dim,activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1=layers.LayerNormalization()\n",
    "        self.layernorm_2=layers.LayerNormalization()\n",
    "        self.layernorm_3=layers.LayerNormalization()\n",
    "        self.supports_masking=True\n",
    "    \n",
    "    def get_config(self):\n",
    "        config=super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def get_casual_attention_mask(self,inputs):\n",
    "        input_shape=tf.shape(inputs)\n",
    "        batch_size,sequence_length=input_shape[0],input_shape[1]\n",
    "        i=tf.range(sequence_length)[:,tf.newaxis]\n",
    "        j=tf.range(sequence_length)\n",
    "        mask=tf.cast(i>=j,dtype=\"int32\")\n",
    "        mask=tf.reshape(mask,(1,input_shape[1],input_shape[1]))\n",
    "        mult=tf.concat(\n",
    "            [tf.expand_dims(batch_size,-1),\n",
    "             tf.constant([1,1],dtype=tf.int32)],axis=0)\n",
    "\n",
    "        return tf.tile(mask,mult)\n",
    "\n",
    "\n",
    "    def call(self,inputs,encorder_outputs,mask=None):\n",
    "        casual_mask=self.get_casual_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask=tf.cast(\n",
    "                mask[:,tf.newaxis,:],dtype=\"int32\"\n",
    "            )\n",
    "            padding_mask=tf.minimum(padding_mask,casual_mask)\n",
    "        else:\n",
    "            padding_mask=mask\n",
    "        attention_output_1=self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=casual_mask\n",
    "        )    \n",
    "        attention_output_1=self.layernorm_1(inputs+attention_output_1)\n",
    "        attention_output_2=self.attention_2(\n",
    "            query=attention_output_1,\n",
    "            value=encorder_outputs,\n",
    "            key=encorder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        attention_output_2=self.layernorm_2(\n",
    "            attention_output_1+attention_output_2\n",
    "        )\n",
    "        proj_output=self.dense_proj(attention_output_2) \n",
    "        return self.layernorm_3(attention_output_2+proj_output)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self,sequence_length,input_dim,output_dim,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings=layers.Embedding(\n",
    "            input_dim=input_dim,output_dim=output_dim)\n",
    "        self.position_embeddings=layers.Embedding(\n",
    "            input_dim=sequence_length,output_dim=output_dim)\n",
    "        self.sequence_length=sequence_length\n",
    "        self.input_dim=input_dim\n",
    "        self.output_dim=output_dim\n",
    "\n",
    "    def call(self,inputs):\n",
    "        length=tf.shape(inputs)[-1]\n",
    "        positions=tf.range(start=0,limit=length,delta=1)\n",
    "        embedded_tokens=self.token_embeddings(inputs)\n",
    "        embedded_positions=self.position_embeddings(positions)\n",
    "\n",
    "        return embedded_tokens + embedded_positions  \n",
    "    \n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.not_equal(inputs, 0)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config=super(PositionalEmbedding,self).get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.framework.ops import disable_eager_execution\n",
    "# disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tried to convert 'x' to a tensor and failed. Error: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[129], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m\n\u001b[0;32m      5\u001b[0m encoder_inputs\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[43mPositionalEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m encoder_outputs\u001b[38;5;241m=\u001b[39mTransformerEncoder(embed_dim,dense_dim,num_heads)(x)\n\u001b[0;32m      9\u001b[0m decorder_inputs\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m,),dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m\"\u001b[39m,name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGerman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[104], line 22\u001b[0m, in \u001b[0;36mPositionalEmbedding.compute_mask\u001b[1;34m(self, inputs, mask)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_mask\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Tried to convert 'x' to a tensor and failed. Error: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "embed_dim=256\n",
    "dense_dim=2048\n",
    "num_heads=8\n",
    "\n",
    "encoder_inputs=keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x=PositionalEmbedding(sequence_length,vocab_size,embed_dim)(encoder_inputs)\n",
    "encoder_outputs=TransformerEncoder(embed_dim,dense_dim,num_heads)(x)\n",
    "\n",
    "decorder_inputs=keras.Input(shape=(None,),dtype=\"int64\",name=\"German\")\n",
    "x=PositionalEmbedding(sequence_length,vocab_size,embed_dim)(decorder_inputs)\n",
    "x=TransformerDecoder(embed_dim,dense_dim,num_heads)(x,encoder_outputs)\n",
    "x=layers.Dropout(0.5)(x)\n",
    "decorder_outputs=layers.Dense(vocab_size,activation=\"softmax\")(x)\n",
    "transformer=keras.Model([encoder_inputs,decorder_inputs],decorder_outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
